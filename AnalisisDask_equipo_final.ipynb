{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da575ef",
   "metadata": {},
   "source": [
    "\n",
    "# Análisis Dask — **Notebook final del equipo (100% comentado)**\n",
    "> Usamos `katalog_gempa.csv` (columnas: `tgl`, `ot`, `lat`, `lon`, `depth`, `mag`, `remark`).  \n",
    "> Construimos `time = to_datetime(tgl + \" \" + ot)` y hacemos EDA + comparativa Pandas vs Dask.  \n",
    "> Escrito en **primera persona plural** como estudiantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# IMPORTS Y OPCIONES GLOBALES\n",
    "# ==============================================\n",
    "# En esta celda importamos lo mínimo necesario y dejamos opciones\n",
    "# para que el notebook sea robusto en Windows/VS Code.\n",
    "# Forzamos a pandas a usar almacenamiento de strings en Python\n",
    "# (evita requerir 'pyarrow') y dejamos todo listo para graficar.\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Importamos librerías base\n",
    "import os  # Para verificar rutas y crear carpetas de salida\n",
    "from time import perf_counter  # Para medir tiempos en nuestros experimentos\n",
    "import pandas as pd  # API de análisis tabular secuencial\n",
    "pd.options.mode.string_storage = \"python\"  # <- evitamos backend 'pyarrow' en strings\n",
    "\n",
    "# Importamos Dask DataFrame y distribución\n",
    "import dask.dataframe as dd  # API tipo pandas pero en paralelo/perezoso\n",
    "from dask.distributed import Client, LocalCluster  # Para cluster local\n",
    "\n",
    "# Importamos matplotlib para gráficos\n",
    "import matplotlib.pyplot as plt  # Gráficos estáticos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# INICIAR / REINICIAR CLUSTER DASK (ROBUSTO)\n",
    "# ==============================================\n",
    "# Nuestro objetivo aquí es levantar un cluster local estable en Windows,\n",
    "# sin widgets (evitamos jinja2/bokeh) y con el dashboard en un puerto libre.\n",
    "# ----------------------------------------------\n",
    "\n",
    "# 1) Cerramos instancias previas si quedaron vivas (evita choques de puertos)\n",
    "for obj in (\"client\", \"cluster\"):\n",
    "    try:\n",
    "        globals().get(obj) and globals()[obj].close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# 2) Creamos un cluster local. Ajustar n_workers/threads_per_worker según CPU.\n",
    "N_WORKERS = 4            # Podemos cambiar a 2/8 según el equipo real\n",
    "THREADS_PER_WORKER = 2   # Ídem\n",
    "cluster = LocalCluster(\n",
    "    n_workers=N_WORKERS,\n",
    "    threads_per_worker=THREADS_PER_WORKER,\n",
    "    processes=True,            # Usamos procesos separados (suele rendir mejor)\n",
    "    dashboard_address=\":0\",    # \":0\" = Dask elige un puerto libre automáticamente\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# 3) NO renderizamos el widget de Client; imprimimos sólo el enlace\n",
    "print(\"Dask dashboard:\", getattr(client, \"dashboard_link\", \"(no disponible)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# LECTURA DEL CSV + CONSTRUCCIÓN DE COLUMNA 'time'\n",
    "# ==============================================\n",
    "# Aquí leemos el archivo 'katalog_gempa.csv' (debe estar junto al notebook\n",
    "# o usamos ruta absoluta). Evitamos 'parse_dates' porque la fecha real\n",
    "# viene separada en 'tgl' (YYYY/MM/DD) y 'ot' (HH:MM:SS.sss).\n",
    "# Luego construimos 'time_str' y de ahí 'time' con to_datetime.\n",
    "# ----------------------------------------------\n",
    "\n",
    "# 1) Definimos la ruta del CSV (si está junto al .ipynb, así basta)\n",
    "CSV_PATH = r\"katalog_gempa.csv\"  # Cambiar a ruta absoluta si hace falta\n",
    "\n",
    "# 2) Verificamos que el archivo exista antes de leerlo\n",
    "print(\"Archivo existe? \", os.path.exists(CSV_PATH), \"→\", CSV_PATH)\n",
    "assert os.path.exists(CSV_PATH), f\"No encontramos el CSV en: {CSV_PATH}\"\n",
    "\n",
    "# 3) Leemos de forma perezosa en Dask con tipos explícitos para evitar sorpresas\n",
    "t0 = perf_counter()\n",
    "df = dd.read_csv(\n",
    "    CSV_PATH,\n",
    "    dtype={\n",
    "        \"tgl\": \"object\",   # fecha en texto\n",
    "        \"ot\": \"object\",    # hora en texto\n",
    "        \"lat\": \"float64\",\n",
    "        \"lon\": \"float64\",\n",
    "        \"depth\": \"float64\",\n",
    "        \"mag\": \"float64\",  # magnitud numérica\n",
    "        \"remark\": \"object\" # descripción/lugar en texto\n",
    "    },\n",
    "    assume_missing=True,   # Permite NaN cuando hay vacíos en columnas numéricas\n",
    "    blocksize=\"64MB\",      # Particionado razonable para equipos personales\n",
    "    # No usamos on_bad_lines para máxima compatibilidad de pandas\n",
    ")\n",
    "t1 = perf_counter()\n",
    "print(f\"Particiones: {df.npartitions} | tiempo lectura (metadatos): {t1 - t0:.2f}s\")\n",
    "\n",
    "# 4) Mostramos columnas y una muestra pequeña (head() sí computa una porción)\n",
    "print(\"Columnas disponibles:\", list(df.columns))\n",
    "print(\"\\\\nMuestra (5 filas):\")\n",
    "display(df.head(5))\n",
    "\n",
    "# 5) Construimos 'time' desde 'tgl' + 'ot' y definimos los nombres estándar\n",
    "#    para usarlos el resto del notebook.\n",
    "df['time_str'] = df['tgl'].astype('string').str.strip() + \" \" + df['ot'].astype('string').str.strip()\n",
    "df['time'] = dd.to_datetime(df['time_str'], errors='coerce')  # naive (sin tz)\n",
    "TIME_COL, MAG_COL, PLACE_COL = 'time', 'mag', 'remark'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# LIMPIEZA Y TIPOS\n",
    "# ==============================================\n",
    "# En esta celda garantizamos que:\n",
    "# - la magnitud sea numérica (coerción convierte texto inválido a NaN),\n",
    "# - el lugar/descripción esté normalizado como texto,\n",
    "# - y descartamos filas sin 'time' o sin 'mag' para análisis consistentes.\n",
    "# ----------------------------------------------\n",
    "\n",
    "# 1) Aseguramos tipos (si algo vino raro del CSV, lo corregimos)\n",
    "df[MAG_COL] = dd.to_numeric(df[MAG_COL], errors='coerce')\n",
    "df[PLACE_COL] = df[PLACE_COL].astype('string').str.strip()\n",
    "\n",
    "# 2) Eliminamos filas que no tengan tiempo o magnitud (datos esenciales)\n",
    "df = df.dropna(subset=[TIME_COL, MAG_COL])\n",
    "\n",
    "# 3) Mostramos tipos y un head tras limpieza para validar que todo tenga sentido\n",
    "print(\"dtypes (lazy):\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nHead tras limpieza:\")\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# DESCRIPTIVOS, TOTAL DE EVENTOS Y TOP UBICACIONES\n",
    "# ==============================================\n",
    "# Nuestro objetivo aquí es producir:\n",
    "# - Estadísticos básicos de 'mag' con describe()\n",
    "# - El total de eventos válidos (filas)\n",
    "# - El Top 10 de ubicaciones según 'remark'\n",
    "# ----------------------------------------------\n",
    "\n",
    "t0 = perf_counter()\n",
    "\n",
    "# 1) Descriptivos de magnitud (Pandas Series tras compute)\n",
    "desc = df[MAG_COL].describe().compute()\n",
    "\n",
    "# 2) Total de eventos limpios\n",
    "total_events = df.shape[0].compute()\n",
    "\n",
    "# 3) Top lugares: IMPORTANTE: primero compute(), luego head(10)\n",
    "top_places = (\n",
    "    df[PLACE_COL]\n",
    "    .value_counts(split_every=8)  # ayuda a escalar cuando hay muchas categorías\n",
    "    .compute()                    # Dask -> Pandas\n",
    "    .head(10)                     # Tomamos los 10 más frecuentes\n",
    ")\n",
    "\n",
    "t1 = perf_counter()\n",
    "\n",
    "print(\"Descriptivos de 'mag':\\n\", desc)\n",
    "print(f\"\\nTotal de eventos: {total_events:,}\")\n",
    "print(\"\\nTop 10 ubicaciones:\\n\", top_places)\n",
    "print(f\"\\nTiempo (descriptivos+conteos): {t1 - t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2dabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# HISTOGRAMA DE MAGNITUDES\n",
    "# ==============================================\n",
    "# Para graficar usamos matplotlib, que trabaja con Pandas. Por eso traemos\n",
    "# la Serie de magnitudes a memoria con compute() y dibujamos el histograma.\n",
    "# ----------------------------------------------\n",
    "\n",
    "# 1) Serie de magnitudes en Pandas\n",
    "mags = df[MAG_COL].dropna().compute()\n",
    "\n",
    "# 2) Histograma\n",
    "plt.figure()\n",
    "plt.hist(mags, bins=30, edgecolor=\"black\")  # No fijamos colores para seguir la guía del profe\n",
    "plt.title(\"Distribución de magnitudes sísmicas\")\n",
    "plt.xlabel(\"Magnitud\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# SERIE TEMPORAL: EVENTOS POR DÍA\n",
    "# ==============================================\n",
    "# Aquí construimos la serie diaria (conteos por día).\n",
    "# Usamos floor('D') para truncar a día, agrupamos y computamos.\n",
    "# ----------------------------------------------\n",
    "\n",
    "daily = (\n",
    "    df[[TIME_COL]]\n",
    "    .assign(day=df[TIME_COL].dt.floor(\"D\"))\n",
    "    .groupby(\"day\")\n",
    "    .size()\n",
    "    .compute()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Gráfico de la serie diaria\n",
    "plt.figure()\n",
    "plt.plot(daily.index, daily.values)\n",
    "plt.title(\"Eventos sísmicos por día\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Cantidad de eventos\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# GRÁFICO: TOP 10 UBICACIONES\n",
    "# ==============================================\n",
    "# Graficamos el top de 'remark' que ya calculamos antes (top_places).\n",
    "# ----------------------------------------------\n",
    "\n",
    "if len(top_places) > 0:\n",
    "    top10 = top_places.sort_values(ascending=True)  # barras horizontales de menor a mayor\n",
    "    plt.figure()\n",
    "    plt.barh(top10.index.astype(str), top10.values)\n",
    "    plt.title(\"Top 10 ubicaciones con más sismos\")\n",
    "    plt.xlabel(\"Conteo de sismos\")\n",
    "    plt.ylabel(\"Ubicación\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay ubicaciones disponibles para graficar (top_places vacío).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40065ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# COMPARATIVA DE RENDIMIENTO: PANDAS VS DASK (EVENTOS POR DÍA)\n",
    "# ==============================================\n",
    "# Replicamos el cálculo de la serie diaria con Pandas y lo comparamos con Dask.\n",
    "# Medimos tiempos con perf_counter y verificamos que los totales coinciden.\n",
    "# ----------------------------------------------\n",
    "\n",
    "# 1) Pandas\n",
    "tp0 = perf_counter()\n",
    "pdf = pd.read_csv(\"katalog_gempa.csv\")  # usamos la misma ruta relativa\n",
    "# Construimos 'time' de la misma forma que en Dask\n",
    "pdf[\"time_str\"] = pdf[\"tgl\"].astype(str).str.strip() + \" \" + pdf[\"ot\"].astype(str).str.strip()\n",
    "pdf[TIME_COL] = pd.to_datetime(pdf[\"time_str\"], errors=\"coerce\")\n",
    "# Limpiamos igual que en Dask\n",
    "pdf = pdf.dropna(subset=[TIME_COL, MAG_COL])\n",
    "# Serie diaria en Pandas\n",
    "pandas_daily = pdf[TIME_COL].dt.floor(\"D\").value_counts().sort_index()\n",
    "tp1 = perf_counter()\n",
    "\n",
    "# 2) Dask (ya tenemos 'daily' arriba). Lo referenciamos directamente.\n",
    "td0 = perf_counter()\n",
    "dask_daily = daily  # ya computado\n",
    "td1 = perf_counter()\n",
    "\n",
    "# 3) Reportamos tiempos y consistencia\n",
    "print(f\"Pandas tiempo: {tp1 - tp0:.2f} s | Filas: {len(pdf):,}\")\n",
    "print(f\"Dask   tiempo: {td1 - td0:.2f} s | Particiones: {df.npartitions}\")\n",
    "print(f\"Total eventos (Pandas vs Dask): {int(pandas_daily.sum())} vs {int(dask_daily.sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fdf0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# GUARDAR RESULTADOS Y FIGURAS\n",
    "# ==============================================\n",
    "# Persistimos algunas salidas para el informe:\n",
    "# - CSV con descriptivos de magnitud\n",
    "# - CSV con eventos por día (Dask)\n",
    "# - PNG con histograma de magnitudes\n",
    "# ----------------------------------------------\n",
    "\n",
    "# 1) Aseguramos carpetas\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "# 2) Guardamos descriptivos y serie diaria\n",
    "desc.to_csv(\"results/descriptivos_magnitud.csv\")\n",
    "daily.to_frame(name=\"count\").to_csv(\"results/eventos_por_dia_dask.csv\")\n",
    "\n",
    "# 3) Guardamos el histograma (re-render simple)\n",
    "plt.figure()\n",
    "plt.hist(mags, bins=30, edgecolor=\"black\")\n",
    "plt.title(\"Distribución de magnitudes sísmicas\")\n",
    "plt.xlabel(\"Magnitud\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/hist_magnitudes.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"Artefactos guardados en ./results y ./figures\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
